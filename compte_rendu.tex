\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{makeidx}
\usepackage{amsmath}
\usepackage[french]{babel}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}

%opening
\makeindex
\title{Projet}
\author{}

\begin{document}

\maketitle

\begin{abstract}
A l’ère du numérique, la collecte de données d’une multitude d’utilisateurs peut devenir très intéressante pour leur faire des suggestions personnalisées de tout type. C’est une chose qu’on rencontre souvent : recommandations d’amis sur Facebook, suggestions d’achats sur Internet, etc.

Nous avons choisi de nous intéresser à la recommandation automatique de films. En effet la recommandation automatique d’un film obéit à plusieurs critères Ajout. Comment concevoir un programme informatique qui prenne en compte tous ces critères pour produire une recommandation personnalisée pertinente ?
Les méthodes proposées lors de ce projet consistent à estimer les notes que mettrait l’utilisateur, afin de trouver les films les plus accordés à ses goûts. Cela se fait en regardant les prédictions des notes les plus hautes. Deux méthodes sont proposées. Pour la première, on suppose qu’il existe une relation linéaire entre l’ensemble des notes données par un utilisateur et la note qu’on cherche à prévoir. Cette méthode consiste à trouver cette relation. Pour ce faire nous utilisons un algorithme, qui sera explicitée par la suite. Nous proposons aussi un second modèle que nous comparons au premier. Dans celui ci, nous supposons que chaque film possède des caractéristiques et chaque utilisateur des préférences, toutes deux liées linéairement. La deuxième méthode consiste donc à déterminer ces préférences et caractéristiques à l’aide d’une utilisation plus avancée de la descente du gradient.
\end{abstract}
\section{Notre approche du problème}
\subsection{Nos données}
\subsubsection{Extraction des données}
Notre projet nous a forcément mener à posséder des données sur des films déjà vu et noté par quelques utilisateurs. 
Cela nous à guider vers une grande base de données appelé `` movielens '' qui est un site communautaire de recommandation de films où les utilisateurs du site notent des films de 1 à 5. 
Plusieurs jeux de données étaient disponibles et différé entre eux selon leur taille, 
nous avons choisi de travailler avec une base de données de 670 utilisateurs et 9125 films. 
On a donc extrait 2 fichiers : l’un contenant les 9125 films avec leurs titres et un numéro attribué , 
l’autre avec les notes des utilisateurs qui était avaient chacun un identifiants, 
le fichier était du type : identifiant de l’utilisateur, id du film qu’il a noté, note.
Ces 2 fichiers étaient donc peu pratiques pour commencer à faire quelque chose avec, 
nous avons donc créer une fonction tableau\_des\_notes() qui permet de ranger toutes ces notes dans un tableau numpy 9125*670 avec en lignes les utilisateurs, 
et en colonnes les films. Quand un utilisateur n’a pas vu un film donc qu’il ne l’a pas noté, 
on insère un `` Nan ''(Not a Number) qui est un `` symbole '' facile à traiter. On appellera ce tableau Y tout au long du projet.


(image de Y)




Pour continuer notre projet a bien, nous avons étudié nos données pour mieux les traiter plus tard.
\subsubsection{Analyse des données}
Dans la logique des choses, notre tableau Y étant bien trop grand pour en tirer des conclusions 
juste à l’œil, nous avons créer des fonctions permettant de faire des sortes de statistiques sur nos données.
Nous avons créer quelques fonctions pour étudier nos données: l’une permettant de compter combien de films chaque utilisateur a vu (cf nbre\_de\_films\_vu\_par\_utilisateur), et l’autre comptant combien de fois avait été noté chaque film (cf nbre\_de\_notes\_par\_film). De ces fonctions, nous avons pu tirer des graphiques nous montrant comment été répartis nos données.


On a remarquer que la grande majorité des films posséder entre 0 et 40 notes(Plus de 8000 sur 9000). Le nombre de notes le plus récurrents étant 2 notes par film. Mais il y a tout de même quelque film qui ont été beaucoup vu sachant que celui qui avait le plus de note en avait 339.

(histogramme nbre de note associés au film)

Dans l’autre sens, en moyenne les utilisateurs ont noté … films, et la majorité est répartis entre blabla et babla.

(autre histogramme)

→ conclusion sur les données ce qu’on peut en tirer …
Notre tableau présente 98,4 \% de NaN. Cela nous fait donc plus de 6 millions de notes à prédire… Dans la prochaine partie, nous verrons donc la méthode permettant de prédire toutes ces notes à partir de très peu de données.

\subsection{Parler de la factorisation de Y : partie de Anthony lors de la présentation orale des POQ}
Expliquer cela avec un exemple simple et des figures de Yf, theta et X
\section{Implémentation de l’algorithme}
\subsection{Introduction de la fonction de coût ( et pourquoi on l'a introduite) avec la norme de frobenius}
Dire que notre but c de minimiser cette fonction en trouvant theta et X qui représentent le mieux possible nos données 
    Explications de Yassine aux POQ sur le fait qu’on fixe aléatoirement theta et X et on les bouges l'un apres l autre plein de fois en faisant diminuer J au fur et a mesure
\subsection{C la que l on parle du principe de la descente du gradient avec les dérivées}
partielles et tt ça : explication de la fonction étape du gradient et descente du gradient non optimisées (sans le gradient) : ce qu'on faisait avant que Valentin nous montre le gradient et toutes ses supers formules au tableau qu on avait pris en photo
Fig avec un bol comme dans le mooc
\section{optimisation}

\subsection{Notations (on pourra mettre cette partie autre part)}

\`{A} remplir et dire aussi que l'on represente par des matrices colonnes les vecteurs et la notation Y y et X x et $y_{.,j}$ preciser qu on travaille avec des matrices réelles
\subsection{Ancien algorithme}

\noindent Une \'{e}tape :\\
\indent Faire simultan\'{e}ment:\\
\indent \indent Pour tout j $\in$ \{1, 2, ..., nf\}:\\
\indent \indent \indent Pour tout k $\in$ \{1, 2, ..., n\} :\\
\indent \indent \indent \indent Affecter \`{a} $x_{j,k}$ la valeur $x_{j,k}-\alpha\frac{\partial J}{\partial x_{j,k}}$\\
\indent Faire simultan\'{e}ment:\\
\indent \indent Pour tout i $\in$ \{1, 2, ..., nu\}:\\
\indent \indent \indent Pour tout k $\in$ \{1, 2, ..., n\} :\\
\indent \indent \indent \indent Affecter \`{a} $\theta_{i,k}$ la valeur $\theta_{i,k}-\alpha\frac{\partial J}{\partial \theta_{i,k}}$\\

\noindent Cet algorithme poss\`{e}de deux fois deux boucles imbriqu\'{e}es ce qui le rend tr\`es lent: 30 secondes par \'{e}tapes avec 10 caract\'{e}ristiques sachant qu'il faut au moins 500 \'{e}tapes pour arriver à une approximation de Y correcte. C'est pourquoi nous voulons l'optimiser.

\subsection{Utilisation du gradient}

Sachant que $\forall j \in \{1, 2, ..., nf\}$ $x_{j}$ est une matrice de dimension $1 * n$, nous pouvons remarquer que faire simultan\'{e}ment:\\

\noindent Pour tout j $\in$ \{1, 2, ..., nf\}:\\
\indent Pour tout k $\in$ \{1, 2, ..., n\} :\\
\indent \indent Affecter \`{a} $x_{j,k}$ la valeur $x_{j,k}-\alpha\frac{\partial J}{\partial x_{j,k}}(\theta, X)$\\

\noindent reviens \`{a} faire simultan\'{e}ment:\\

\noindent Pour tout j $\in$ \{1, 2, ..., nu\}:\\
\indent $x_{j} := x_{j}-\alpha
\begin{pmatrix}
\frac{\partial J}{\partial x_{j,1}}(\theta, X) & \frac{\partial J}{\partial x_{j,2}}(\theta, X) & \cdots & \frac{\partial J}{\partial x_{j,n}}(\theta, X)
\end{pmatrix}$\\

\noindent Or, $\begin{pmatrix}
\frac{\partial J}{\partial x_{j,1}}(\theta, X) & \frac{\partial J}{\partial x_{j,2}}(\theta, X) & \cdots & \frac{\partial J}{\partial x_{j,n}}(\theta, X)
\end{pmatrix}
= (\nabla_{x_{j}^T} J(\theta, X))^{T}$\\

\noindent Et il n'y a ainsi plus de boucles imbriqu\'{e}es. Nous nous posons maintenant la question comment faire une fonction renvoyant rapidement $(\nabla_{x_{j}^T} J(\theta, X))^{T}$ afin de pouvoir utiliser cette astuce.\\

\noindent Nous avons:\\\\
$J(\theta, X)=\frac{1}{2}\displaystyle\sum_{\substack{i,j \\ y_{i,j} \ne nan}}(\theta_{i}x_{j}^{T}-y_{i,j})^{2}$

$=\frac{1}{2}\displaystyle\sum_{k=1}^{nf}\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}$\\

\noindent Qu'est ce que $\tilde{y}_{.,k}$ et $\tilde{\theta}$? En fait $\tilde{y}_{.,k}$ est le vecteur $y_{.,k}$ auquel nous avons enlev\'{e} les composantes nan : par exemple si $y_{.,k}=
\begin{pmatrix}
2\\3\\nan\\5\\nan
\end{pmatrix}$
alors $\tilde{y}_{.,k}=
\begin{pmatrix}
2\\3\\5
\end{pmatrix}$. Et si nous avons pour cela retir\'{e} les lignes $l_{1}, l_{2}, \cdots$ et $l{u}$ de y, nous retirons les m\^{e}mes lignes de $\theta$ pour former $\tilde{\theta}$.  D'o\`{u} $\tilde{\theta}$ d\'{e}pend de $y_{.,k}$ et donc de k.\\

\noindent Et alors, nous en d\'{e}duisons, $\forall j \in$ \{1, 2, ..., nf\} :\\\\
$\nabla_{x_{j}^T} J(\theta, X)=
\begin{pmatrix}
\displaystyle\frac{\partial \displaystyle\sum_{k=1}^{nf}\frac{1}{2}\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,1}^{T}}\\
\displaystyle\frac{\partial \displaystyle\sum_{k=1}^{nf}\frac{1}{2}\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,2}^{T}}\\
\vdots\\
\displaystyle\frac{\partial \displaystyle\sum_{k=1}^{nf}\frac{1}{2}\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,n}^{T}}
\end{pmatrix}
=
\begin{pmatrix}
\displaystyle\sum_{k=1}^{nf}
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,1}^{T}}\\
\displaystyle\sum_{k=1}^{nf}
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,2}^{T}}\\
\vdots\\
\displaystyle\sum_{k=1}^{nf}
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{k}^{T}-\tilde{y}_{.,k}\Vert^{2}}{\partial x_{j,n}^{T}}
\end{pmatrix}$\\
$
=
\begin{pmatrix}
\displaystyle
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert^{2}}{\partial x_{j,1}^{T}}\\
\displaystyle
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert^{2}}{\partial x_{j,2}^{T}}\\
\vdots\\
\displaystyle
\frac{1}{2}\frac{\partial\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert^{2}}{\partial x_{j,n}^{T}}
\end{pmatrix}
=
\displaystyle
\nabla_{x_{j}^T}\frac{1}{2}\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert^{2}
$\\\\
Et nous pouvons calculer $(\nabla_{x_{j}^{T}}\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert)$ gr\^{a}ce \`{a} une formule qui est prouvé en annexe: $\forall n \in \mathbb{N}^{*} \forall p \in \mathbb{N}^{*}$ si A est une matrice de dimension $p*n$, x une matrice colonne de dimension $n*1$ et b une matrice colonne de dimension $p*1$ alors on a $\nabla_{x}\Vert Ax-b \Vert = A^{T}(Ax-b)$.\\\\
D'o\`{u}, $\nabla_{x_{j}^{T}}\Vert\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}\Vert = \tilde{\theta}^{T}(\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j})=\nabla_{x_{j}^T} J(\theta, X)$.\\\\
Et ceci est rapidement calculable gr\^{a}ce aux fonctions de la biblioth\`eque Numpy que nous utilisons pour notre programme.\\\\
Un raisonnement similaire am\`ene \`a faire de m\^eme avec la partie de l'ancien algorithme modifiant $\theta$ ce qui nous donne ce nouvel algorithme beaucoup plus rapide :\\

\noindent Une \'{e}tape :\\
\indent Faire simultan\'{e}ment:\\
\indent \indent Pour tout j $\in$ \{1, 2, ..., nf\}:\\
\indent \indent \indent Affecter \`{a} $x_{j,k}$ la valeur $x_{j,k}-\alpha(\tilde{\theta}^{T}(\tilde{\theta}x_{j}^{T}-\tilde{y}_{.,j}))^{T}$\\
\indent Faire simultan\'{e}ment:\\
\indent \indent Pour tout i $\in$ \{1, 2, ..., nu\}:\\
\indent \indent \indent Affecter \`{a} $\theta_{i,k}$ la valeur $\theta_{i,k}-\alpha(\tilde{X}^{T}(\tilde{X}\theta_{i}^{T}-\tilde{y}_{i}))^{T}$\\

\section{Mise en place de la recommandation pour un utilisateur spécifique}
\section{conclusion}
\appendix
\section{Preuve}
\begin{align*}
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= [\nabla_{x} \frac{1}{2}(Ax - b).(Ax - b)]_{j}\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= [\nabla_{x} \frac{1}{2}(\sum^{n}_{i = 1} (\sum^{p}_{k = 1} A_{i, k}x_{k})^{2} - 2b_{i}(\sum^{p}_{k = 1} A_{i, k}x_{k}) + b_{i}^{2})]_{j}\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= \frac{\partial\frac{1}{2}(\sum^{n}_{i = 1} (\sum^{p}_{k = 1} A_{i, k}x_{k})^{2} - 2b_{i}(\sum^{p}_{k = 1} A_{i, k}x_{k}) + b_{i}^{2})}{\partial x_{j}}\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= \frac{1}{2}(\sum^{n}_{i = 1} 2(\sum^{p}_{k = 1} A_{i, k}x_{k})A_{i, j} - 2b_{i} A_{i, j})\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= (\sum^{n}_{i = 1} (\sum^{p}_{k = 1} A_{i, k}x_{k})A_{i, j} - b_{i} A_{i, j})\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= (\sum^{n}_{i = 1} A_{i, j}(\sum^{p}_{k = 1} A_{i, k}x_{k}) - b_{i} )\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= \sum^{n}_{i = 1} A^{T}_{j, i}[Ax - b]_{i}\\
[\nabla_{x} \frac{1}{2}||Ax - b||^{2}_{2}]_{j} &= [A^{T}(Ax - b)]_{j}
\end{align*}
\end{document}
